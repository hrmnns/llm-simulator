{
  "project": "LLM-Explorer",
  "version": "3.37",
  "scenarios": [
    {
      "id": "polysemie-schloss-labor",
      "name": "Mehrdeutigkeit: Das Schloss",
      "input_prompt": "Das Schloss war f√ºr den Einbrecher kein Hindernis.",
      "explanation": "Steuerung: Head 3 (Logik/Mechanik) selektiert T√ºrschloss via 'Einbrecher'. Head 1 (Semantik) selektiert Prachtbau via 'Hindernis'.",
      "phase_0_tokenization": {
        "tokens": [
          {
            "id": "0",
            "text": "Das"
          },
          {
            "id": "1",
            "text": "Schloss"
          },
          {
            "id": "2",
            "text": "war"
          },
          {
            "id": "3",
            "text": "f√ºr"
          },
          {
            "id": "4",
            "text": "den"
          },
          {
            "id": "5",
            "text": "Einbrecher"
          },
          {
            "id": "6",
            "text": "kein"
          },
          {
            "id": "7",
            "text": "Hindernis"
          }
        ]
      },
      "phase_1_embedding": {
        "token_vectors": [
          {
            "token_index": 0,
            "base_vector": [
              0.1,
              0.1
            ],
            "positional_vector": [
              -0.5,
              0.0
            ]
          },
          {
            "token_index": 1,
            "base_vector": [
              0.5,
              0.5
            ],
            "positional_vector": [
              -0.3,
              0.0
            ]
          },
          {
            "token_index": 5,
            "base_vector": [
              0.8,
              0.7
            ],
            "positional_vector": [
              0.3,
              0.0
            ]
          },
          {
            "token_index": 7,
            "base_vector": [
              0.1,
              0.1
            ],
            "positional_vector": [
              0.6,
              0.0
            ]
          }
        ]
      },
      "phase_2_attention": {
        "attention_profiles": [
          {
            "id": "context-check",
            "label": "Kontext-Analyse",
            "rules": [
              {
                "head": 1,
                "source": "1",
                "target": "7",
                "strength": 1.2
              },
              {
                "head": 3,
                "source": "1",
                "target": "5",
                "strength": 1.4
              }
            ]
          }
        ]
      },
      "phase_3_ffn": {
        "activation_profiles": [
          {
            "ref_profile_id": "context-check",
            "activations": [
              {
                "label": "Funktional",
                "activation": 0.50,
                "linked_head": 3,
                "color": "#10b981",
                "icon": "‚öôÔ∏è",
                "target_tokens": [
                  "T√ºrschloss",
                  "Sperre",
                  "Bolzen"
                ]
              },
              {
                "label": "Akademisch",
                "activation": 0.50,
                "linked_head": 1,
                "color": "#3b82f6",
                "icon": "üèõÔ∏è",
                "target_tokens": [
                  "Prachtbau",
                  "Residenz",
                  "Palast"
                ]
              }
            ]
          }
        ]
      },
      "phase_4_decoding": {
        "outputs": [
          {
            "label": "T√ºrschloss",
            "logit": 4.2,
            "type": "Funktional",
            "noise_sensitivity": 0.8
          },
          {
            "label": "Prachtbau",
            "logit": 4.2,
            "type": "Akademisch",
            "noise_sensitivity": 0.8
          },
          {
            "label": "Das",
            "logit": 3.8,
            "type": "Neutral"
          }
        ]
      }
    },
    {
      "id": "hallucination-lab-001",
      "name": "Halluzinations-Labor: Der Logik-Verlust",
      "input_prompt": "Die Hauptstadt von Frankreich ist",
      "explanation": "Steuerung: Head 3 (Logik) aktiviert Geographie-Fakten. Head 4 (Zufall) provoziert Halluzinationen.",
      "phase_0_tokenization": {
        "tokens": [
          {
            "id": "0",
            "text": "Die"
          },
          {
            "id": "1",
            "text": "Hauptstadt"
          },
          {
            "id": "2",
            "text": "von"
          },
          {
            "id": "3",
            "text": "Frankreich"
          },
          {
            "id": "4",
            "text": "ist"
          }
        ]
      },
      "phase_1_embedding": {
        "token_vectors": [
          {
            "token_index": 0,
            "base_vector": [
              0.1,
              0.1
            ],
            "positional_vector": [
              -0.4,
              0.0
            ]
          },
          {
            "token_index": 1,
            "base_vector": [
              0.4,
              0.5
            ],
            "positional_vector": [
              -0.2,
              0.0
            ]
          },
          {
            "token_index": 3,
            "base_vector": [
              0.8,
              0.8
            ],
            "positional_vector": [
              0.2,
              0.0
            ]
          },
          {
            "token_index": 4,
            "base_vector": [
              0.1,
              0.1
            ],
            "positional_vector": [
              0.4,
              0.0
            ]
          }
        ]
      },
      "phase_2_attention": {
        "attention_profiles": [
          {
            "id": "entropy-mode",
            "label": "Fakten-Extraktion",
            "rules": [
              {
                "head": 3,
                "source": "4",
                "target": "3",
                "strength": 1.4
              },
              {
                "head": 4,
                "source": "4",
                "target": "0",
                "strength": 0.8
              }
            ]
          }
        ]
      },
      "phase_3_ffn": {
        "activation_profiles": [
          {
            "ref_profile_id": "entropy-mode",
            "activations": [
              {
                "label": "Geographie",
                "activation": 0.50,
                "linked_head": 3,
                "color": "#3b82f6",
                "icon": "üåç",
                "target_tokens": [
                  "Paris",
                  "Lyon",
                  "Marseille"
                ]
              },
              {
                "label": "Zufall",
                "activation": 0.50,
                "linked_head": 4,
                "color": "#f97316",
                "icon": "ü•¥",
                "target_tokens": [
                  "Bananen-Republik",
                  "Zitroneneis"
                ]
              }
            ]
          }
        ]
      },
      "phase_4_decoding": {
        "outputs": [
          {
            "label": "Paris",
            "logit": 4.5,
            "type": "Geographie",
            "noise_sensitivity": 0.2
          },
          {
            "label": "Bananen-Republik",
            "logit": 4.2,
            "type": "Zufall",
            "noise_sensitivity": 0.9
          },
          {
            "label": "Zitroneneis",
            "logit": 3.8,
            "type": "Zufall",
            "noise_sensitivity": 0.9
          }
        ]
      }
    },
    {
      "id": "poetry-generator-001",
      "name": "Poesie-Generator: Reim vs. Botanik",
      "input_prompt": "Rosen sind rot, Veilchen sind",
      "explanation": "Steuerung: Head 1 (Semantik/Fakten) vs. Head 4 (Struktur/Reim).",
      "phase_0_tokenization": {
        "tokens": [
          {
            "id": "0",
            "text": "Rosen"
          },
          {
            "id": "1",
            "text": "sind"
          },
          {
            "id": "2",
            "text": "rot"
          },
          {
            "id": "3",
            "text": ","
          },
          {
            "id": "4",
            "text": "Veilchen"
          },
          {
            "id": "5",
            "text": "sind"
          }
        ]
      },
      "phase_1_embedding": {
        "token_vectors": [
          {
            "token_index": 0,
            "base_vector": [
              0.8,
              -0.2
            ],
            "positional_vector": [
              -0.6,
              0.0
            ]
          },
          {
            "token_index": 1,
            "base_vector": [
              0.0,
              0.0
            ],
            "positional_vector": [
              -0.4,
              0.0
            ]
          },
          {
            "token_index": 2,
            "base_vector": [
              0.7,
              0.5
            ],
            "positional_vector": [
              -0.2,
              0.0
            ]
          },
          {
            "token_index": 3,
            "base_vector": [
              0.0,
              0.0
            ],
            "positional_vector": [
              0.0,
              0.0
            ]
          },
          {
            "token_index": 4,
            "base_vector": [
              0.6,
              -0.4
            ],
            "positional_vector": [
              0.3,
              0.0
            ]
          },
          {
            "token_index": 5,
            "base_vector": [
              0.0,
              0.0
            ],
            "positional_vector": [
              0.6,
              0.0
            ]
          }
        ]
      },
      "phase_2_attention": {
        "attention_profiles": [
          {
            "id": "poetic-mode",
            "label": "Kontext: Stil-Analyse",
            "rules": [
              {
                "head": 1,
                "source": "5",
                "target": "4",
                "strength": 1.2,
                "explanation": "Semantik: Fokus auf die Pflanze 'Veilchen'."
              },
              {
                "head": 4,
                "source": "5",
                "target": "2",
                "strength": 1.4,
                "explanation": "Struktur: Suche nach einem Reimwort zu 'rot'."
              }
            ]
          }
        ]
      },
      "phase_3_ffn": {
        "activation_profiles": [
          {
            "ref_profile_id": "poetic-mode",
            "activations": [
              {
                "label": "Wissenschaft",
                "activation": 0.50,
                "linked_head": 1,
                "color": "#10b981"
              },
              {
                "label": "Poetisch",
                "activation": 0.50,
                "linked_head": 4,
                "color": "#8b5cf6"
              }
            ]
          }
        ]
      },
      "phase_4_decoding": {
        "outputs": [
          {
            "label": "blau",
            "logit": 5.1,
            "type": "Poetisch"
          },
          {
            "label": "violett",
            "logit": 5.0,
            "type": "Wissenschaft"
          }
        ]
      }
    },
    {
      "id": "gender-bias-doctor-001",
      "name": "Gender-Bias: Die √Ñrztin",
      "input_prompt": "Die √Ñrztin sagte dem Pfleger, dass",
      "explanation": "Head 1 steuert Maskulin, Head 3 steuert Feminin.",
      "phase_0_tokenization": {
        "tokens": [
          {
            "id": "0",
            "text": "Die"
          },
          {
            "id": "1",
            "text": "√Ñrztin"
          },
          {
            "id": "2",
            "text": "sagte"
          },
          {
            "id": "3",
            "text": "dem"
          },
          {
            "id": "4",
            "text": "Pfleger"
          },
          {
            "id": "5",
            "text": "dass"
          }
        ]
      },
      "phase_1_embedding": {
        "token_vectors": [
          {
            "token_index": 0,
            "base_vector": [
              0.1,
              0.5
            ],
            "positional_vector": [
              -0.5,
              0.0
            ]
          },
          {
            "token_index": 1,
            "base_vector": [
              -0.2,
              0.9
            ],
            "positional_vector": [
              -0.3,
              0.1
            ]
          },
          {
            "token_index": 2,
            "base_vector": [
              0.1,
              0.0
            ],
            "positional_vector": [
              -0.1,
              0.0
            ]
          },
          {
            "token_index": 3,
            "base_vector": [
              0.5,
              -0.1
            ],
            "positional_vector": [
              0.1,
              0.0
            ]
          },
          {
            "token_index": 4,
            "base_vector": [
              0.9,
              -0.2
            ],
            "positional_vector": [
              0.3,
              -0.1
            ]
          },
          {
            "token_index": 5,
            "base_vector": [
              0.0,
              0.0
            ],
            "positional_vector": [
              0.5,
              0.0
            ]
          }
        ]
      },
      "phase_2_attention": {
        "attention_profiles": [
          {
            "id": "biased-mode",
            "label": "Kontext",
            "rules": [
              {
                "head": 1,
                "source": "5",
                "target": "4",
                "strength": 1.0
              },
              {
                "head": 3,
                "source": "5",
                "target": "1",
                "strength": 1.0
              }
            ]
          }
        ]
      },
      "phase_3_ffn": {
        "activation_profiles": [
          {
            "ref_profile_id": "biased-mode",
            "activations": [
              {
                "label": "Maskulin",
                "activation": 0.50,
                "linked_head": 1,
                "color": "#3b82f6"
              },
              {
                "label": "Feminin",
                "activation": 0.50,
                "linked_head": 3,
                "color": "#f472b6"
              }
            ]
          }
        ]
      },
      "phase_4_decoding": {
        "outputs": [
          {
            "label": "er",
            "logit": 5.0,
            "type": "Maskulin"
          },
          {
            "label": "sie",
            "logit": 5.0,
            "type": "Feminin"
          }
        ]
      }
    }
  ]
}